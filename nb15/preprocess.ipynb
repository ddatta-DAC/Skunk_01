{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np \n",
    "import sys\n",
    "sys.path.append('./../..')\n",
    "sys.path.append('./..')\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from common_utils import utils \n",
    "\n",
    "def normalize_minmax(value, _max, _min):\n",
    "    return (value - _min)/(_max -_min)\n",
    "\n",
    "def replace_with_id( row , ref_dict, col):\n",
    "    value =  row[col]\n",
    "    if value not in ref_dict.keys():\n",
    "        row[col] = None\n",
    "    else:\n",
    "        row[col] = ref_dict[value]\n",
    "    return row\n",
    "\n",
    "def preprocess_data(df_normal, df_anomalies, categorical_columns, real_value_columns):\n",
    "    master_df = df_normal.append(df_anomalies,ignore_index=True)\n",
    "    master_df = master_df.dropna()\n",
    "    # Delete columns with a single value\n",
    "    for col in master_df.columns:\n",
    "        count = len(set(master_df[col]))\n",
    "        if count == 1 and col in categorical_columns:\n",
    "            print(col, count)\n",
    "            try:\n",
    "                del master_df[col]\n",
    "            except:\n",
    "                pass\n",
    "            categorical_columns.remove(col)\n",
    "\n",
    "    # Order the columns\n",
    "    label_col = 'label'\n",
    "    ordered_columns = categorical_columns + real_value_columns + [label_col]\n",
    "    master_df = master_df[ordered_columns]\n",
    "    \n",
    "    single_value_cols = []\n",
    "    target_columns = list(categorical_columns)\n",
    "    entity_count = {}\n",
    "\n",
    "    for i in tqdm(range(len(target_columns))):\n",
    "\n",
    "        column = target_columns[i]\n",
    "        valid_values = sorted(set(master_df[column]))\n",
    "        val2id_dict = { \n",
    "            e[1]:e[0] for e in enumerate(valid_values,0)\n",
    "        }\n",
    "        print(' --> ', column, 'Number of valid values', len(val2id_dict))\n",
    "\n",
    "        if len(val2id_dict) == 1 :\n",
    "            single_value_cols.append(column)\n",
    "            continue\n",
    "\n",
    "        entity_count[column] = len(val2id_dict)\n",
    "\n",
    "        master_df = master_df.parallel_apply(\n",
    "            replace_with_id,\n",
    "            axis=1,\n",
    "            args = (val2id_dict, column,)\n",
    "        )\n",
    "    cat_domain_dims = entity_count\n",
    "    oneHot_encoder_list = []\n",
    "    idx = 0\n",
    "    for _ , dim in cat_domain_dims.items():\n",
    "        if dim ==2 :\n",
    "            _drop = 'first'\n",
    "        else:\n",
    "            _drop = None\n",
    "        name = \"oh_\"+str(idx) \n",
    "        oh_encoder = OneHotEncoder(\n",
    "            np.reshape( list(range(dim)),[1,-1] ),\n",
    "            sparse=False,\n",
    "            drop=_drop\n",
    "        ) \n",
    "        oneHot_encoder_list.append((name, oh_encoder, [idx]))\n",
    "        idx +=1\n",
    "    column_encoder = ColumnTransformer(\n",
    "        oneHot_encoder_list\n",
    "    )\n",
    "\n",
    "    num_categories = len(cat_domain_dims)\n",
    "    samples_np = master_df.values\n",
    "    samples_cat_part = samples_np[:,:num_categories]\n",
    "    samples_real_part = samples_np[:,num_categories:]\n",
    "    onehot_xformed = column_encoder.fit_transform(samples_cat_part)\n",
    "    samples = np.concatenate([onehot_xformed, samples_real_part],axis=1)\n",
    "    column_names = []\n",
    "    for cat,dim in cat_domain_dims.items():\n",
    "        if dim > 2:\n",
    "            column_names += [ cat+str(_) for _ in range(dim)]\n",
    "        else:\n",
    "            column_names += [ cat+str(1) ]\n",
    "    column_names += real_value_columns\n",
    "    column_names += [label_col]\n",
    "    oh_master_df = pd.DataFrame(samples, columns = column_names )\n",
    "    return oh_master_df, categorical_columns, real_value_columns, cat_domain_dims\n",
    "\n",
    "\n",
    "\n",
    "# Create train test sets \n",
    "def create_sets(\n",
    "    df,\n",
    "    save_dir,\n",
    "    real_value_columns,\n",
    "    num_sets=10,\n",
    "    label_col = 'label',\n",
    "    anomaly_label = 1,\n",
    "    test_ratio = 0.5\n",
    "):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    normal_data = df.loc[df[label_col]!=anomaly_label]\n",
    "    for set_id in range(1, num_sets+1):\n",
    "        train, test = train_test_split(normal_data,test_size=test_ratio)\n",
    "        anom =  pd.DataFrame(df.loc[df[label_col]==anomaly_label])\n",
    "        # Save data \n",
    "        train_file = 'train_data_onehot.csv'\n",
    "        test_file = 'test_data_onehot.csv'\n",
    "        \n",
    "        \n",
    "        # Normalize the continuous values\n",
    "        \n",
    "        for column in real_value_columns:\n",
    "            _min = min(train[column])\n",
    "            _max = max(train[column])\n",
    "            if _max == _min: \n",
    "                continue\n",
    "            train[column] = train[column].parallel_apply(normalize_minmax, args= (_max,_min, ))\n",
    "            test[column] = test[column].parallel_apply(normalize_minmax, args= (_max,_min, ))\n",
    "            anom[column] = anom[column].parallel_apply(normalize_minmax, args= (_max,_min, ))\n",
    "        del train[label_col]   \n",
    "        del test[label_col] \n",
    "        del anom[label_col] \n",
    "        \n",
    "        # Save the files \n",
    "        path = os.path.join(save_dir, 'set_' + str(set_id)  )\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        train_fp = os.path.join( path, 'train.npz')\n",
    "        test_fp = os.path.join( path, 'test.npz')\n",
    "        anom_fp = os.path.join( path, 'anom.npz')\n",
    "        sparse.save_npz(train_fp, sparse.csr_matrix(train.values))\n",
    "        sparse.save_npz(test_fp, sparse.csr_matrix(test.values))\n",
    "        sparse.save_npz(anom_fp, sparse.csr_matrix(anom.values))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from common_utils import utils "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------- \n",
    "# UNSW NB 15 data \n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcip\n",
      "dstip\n",
      "dsport\n",
      "sport\n",
      "stime\n",
      "ltime\n",
      "5009 Counter({'Analysis': 2000, 'Backdoor': 1746, 'Shellcode': 1133, 'Worms': 130})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "column_headers_file = './NUSW-NB15_features.csv'\n",
    "features_df = pd.read_csv(column_headers_file,index_col=None,encoding='latin-1')\n",
    "features_df.columns\n",
    "\n",
    "features_df = features_df[['Name','Type ']]\n",
    "features_df = features_df.rename(columns={'Type ':'Type'})\n",
    "features_df['Name']=features_df['Name'].apply(str.lower)\n",
    "features_df['Type']=features_df['Type'].apply(str.lower)\n",
    "features_df = features_df.append({'Name':'rate','Type':'float'},ignore_index=True)\n",
    "\n",
    "invalid_columns = [\n",
    "    'srcip','dstip','dsport','sport','stime','ltime'\n",
    "]\n",
    "\n",
    "columns = list(features_df['Name'])\n",
    "for r in invalid_columns:\n",
    "    print(r)\n",
    "    columns.remove(r)\n",
    "columns =[ _.replace(' ','') for _ in columns]\n",
    "\n",
    "data_df = pd.read_csv('UNSW_NB15_training-set.csv', index_col=None)\n",
    "replace_ = {\n",
    "'dintpkt':'sinpkt',\n",
    "'sintpkt':'dinpkt',\n",
    "'smeansz':'smean',\n",
    "'dmeansz': 'dmean',\n",
    "'res_bdy_len' :'response_body_len',\n",
    "'ct_src_ ltm': 'ct_src_ltm'\n",
    "}\n",
    "features_df.replace(to_replace = replace_,inplace=True)\n",
    "\n",
    "normal_classes = ['Normal']\n",
    "anomaly_classes = [ _ for _ in set(data_df['attack_cat']) if _ not in ['Normal','Generic','Exploits','Fuzzers','DoS','Reconnaissance']]\n",
    "df_normal = data_df.loc[data_df['attack_cat'].isin(normal_classes)]\n",
    "df_anomaly = data_df.loc[data_df['attack_cat'].isin(anomaly_classes)]\n",
    "print(len(df_anomaly), Counter(df_anomaly['attack_cat']))\n",
    "df_normal['label'] = 0\n",
    "df_anomaly['label'] = 1\n",
    "\n",
    "if len(df_normal) < len(df_anomaly):                  \n",
    "    df_anomaly = df_anomaly.sample (n=int(len(df_normal)))\n",
    "master_df = df_normal.append(df_anomaly,ignore_index=True)\n",
    "\n",
    "del master_df['id']\n",
    "del master_df['attack_cat']\n",
    "master_df = master_df.dropna()\n",
    "real_value_columns = []\n",
    "categorical_columns = []\n",
    "binary_columns = []\n",
    "\n",
    "for column in master_df.columns:\n",
    "    if column in list(features_df['Name']):\n",
    "        _type = list(features_df.loc[features_df['Name']==column]['Type'])[0]\n",
    "        if _type =='integer' or _type =='float':\n",
    "            real_value_columns.append(column)\n",
    "        elif _type =='binary':\n",
    "            if column =='label':\n",
    "                continue\n",
    "            binary_columns.append(column)\n",
    "        elif _type=='nominal':\n",
    "            categorical_columns.append(column)\n",
    "categorical_columns = categorical_columns + binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -->  proto Number of valid values 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:04<00:17,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -->  service Number of valid values 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:08<00:13,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -->  state Number of valid values 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:13<00:08,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -->  is_ftp_login Number of valid values 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:16<00:04,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -->  is_sm_ips_ports Number of valid values 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:19<00:00,  3.99s/it]\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:76: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories != 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories == 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:76: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories != 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories == 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:76: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories != 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories == 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:76: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories != 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories == 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:76: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories != 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.categories == 'auto':\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/ipykernel_launcher.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/ipykernel_launcher.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "oh_master_df, categorical_columns, real_value_columns, cat_domain_dims =  preprocess_data(df_normal, df_anomaly, categorical_columns, real_value_columns)\n",
    "save_dir = 'processed_sets'\n",
    "create_sets(\n",
    "    df = oh_master_df,\n",
    "    real_value_columns = real_value_columns,\n",
    "    save_dir = save_dir,\n",
    "    test_ratio=0.3\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Write out the dimensionality of the columns into a csv file\n",
    "# ============================\n",
    "col_name_list = []\n",
    "dimensionality = []\n",
    "data =[]\n",
    "for c,v in cat_domain_dims.items():\n",
    "    col_name_list.append(c)\n",
    "    dimensionality.append(v)\n",
    "    data.append((c,v)) \n",
    "df_data_dimensions = pd.DataFrame(\n",
    "    data = data,\n",
    "    columns=['column','dimension']\n",
    ")\n",
    "\n",
    "# Save metadata\n",
    "f_name = 'data_dimensions.csv'\n",
    "f_path = os.path.join(save_dir, f_name )\n",
    "df_data_dimensions.to_csv(f_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
